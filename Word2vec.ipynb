{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word2vec.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1mYeuHyp2R6bbA95LKJ-K83vFNbLexJAU","authorship_tag":"ABX9TyPdOJrJbW66hSnMqjaP2AsH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KRp2i5IXkpfv","colab_type":"text"},"source":["## Word Embedding\n","\n","Word Embeddings: Distributional vectors, also called word embeddings, are based on the so-called distributional hypothesis — words appearing within similar context possess similar meaning. Word embeddings are pre-trained on a task where the objective is to predict a word based on its context, typically using a shallow neural network. "]},{"cell_type":"markdown","metadata":{"id":"Xg0qxx6OTb_m","colab_type":"text"},"source":["## Word2vec\n","\n","Document was a vector in TFIDF but in world2vec ,it is a matrix becuase we convert in onehotencode to all words in a a document.\n","what is the relationship between words.Can we solve semantic like relationship in words by TDIDF.I think no. We can do this with the help of word2vec.\n","what does it do. Assign number to each words. and these numbers have meaning. We need to train a model word2vec to get this semantic relationship. It try to find the neighbourhood .\n","We find the probability of the word being nearby word.\n","what is near or neighbourhood word in document. It measn just near word left or right in document. we use window size to represent near word. id window size is 1 it measn wr consider just left and right 1 word and if windows size 2 then we conside near 2 words. \n","\n","word2vec embeddings: we use 2 methoths.\n","1. CBOW(contineous bag of word) : we predict the target word by using context words. as per below image.\n"]},{"cell_type":"markdown","metadata":{"id":"lk0QY1JIk5ow","colab_type":"text"},"source":["<img src=\"https://raw.githubusercontent.com/teja/Machine_Learning/master/Images/cbow.PNG\" width=\"440\" height=\"240\" align=\"left\"/>"]},{"cell_type":"markdown","metadata":{"id":"hYrB0JNdk1gr","colab_type":"text"},"source":["2. Skip-Gram : we predict the contexts word by using target word.It have 1 input,i output and 1 hidden layer. Input vector size is equal to no of unique words or no of vacabulary.Lets If we have 500 unique words and we put input word in the form of one hot encode then each word input size would be (500,1). we can select No of neurons as per our choice and output of hidden layer wll be feed into output layer. No of output from output layer is same as expected no of unique words. Output will be probability of neighbourhood words , It measn high probability measn near by word and low probability measn non-neighbour word. \n","Lets if we have windows size of 5 then we will get total 10 words which will have high probability and remaining will have very low probability. Word2Vec uses Softmax as the last layer and Cross-Entropy loss. Now, the problem with Softmax is that the gradient is dependent on the summation across all classes.So when we try to train the such model where we only get very less amount of high probability then model will try to learn the always low probability. But Our Model should predict the positive probability to get the neighbour word. So we use negative sampling so that our model can predict the positive probability aslo. In negative sampling we only update the few weights. measn weights corresponding to positive outputs and very small number of weights of negative outputs. After traiining we discard the output layer and for each input word we get N words where N is the no of neurons. Also N is called as embedding size."]},{"cell_type":"markdown","metadata":{"id":"DeDYCB1Wk6uZ","colab_type":"text"},"source":["<img src=\"https://raw.githubusercontent.com/atulpatelDS/Machine_Learning/master/Images/skip-gram.PNG\" width=\"440\" height=\"240\" align=\"left\"/>"]},{"cell_type":"markdown","metadata":{"id":"2iGEkmy8ZTyB","colab_type":"text"},"source":["A word embedding model is a model that can provide numerical vectors for a given word. Using the Gensim’s downloader API, we can download pre-built word embedding models like word2vec, fasttext, GloVe and ConceptNet. These are built on large corpuses of commonly occurring text data such as wikipedia, google news etc.\n","\n","However, if we are working in a specialized niche such as technical documents, we may not able to get word embeddings for all the words. So, in such cases its desirable to train our own model.\n","\n","Gensim’s Word2Vec implementation let’s train our own word embedding model for a given corpus."]},{"cell_type":"code","metadata":{"id":"_WIr04dykoJY","colab_type":"code","colab":{}},"source":["import gensim\n","import pandas as pd\n","import re,string\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wHlRSrQwkoLx","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv(\"https://raw.githubusercontent.com/atulpatelDS/Data_Files/master/Bag_of_Words/word2vec_nlp/unlabeledTrainData.tsv.zip\",\n","                      header=0,delimiter=\"\\t\",quoting=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJmhuqyCoQOk","colab_type":"code","outputId":"12677add-dcdd-4b68-f5c1-1a8144dffdeb","executionInfo":{"status":"ok","timestamp":1591075230408,"user_tz":-330,"elapsed":1214,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["dataset.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"9999_0\"</td>\n","      <td>\"Watching Time Chasers, it obvious that it was...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"45057_0\"</td>\n","      <td>\"I saw this film about 20 years ago and rememb...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"15561_0\"</td>\n","      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"7161_0\"</td>\n","      <td>\"I went to see this film with a great deal of ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"43971_0\"</td>\n","      <td>\"Yes, I agree with everyone on this site this ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id                                             review\n","0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n","1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n","2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n","3   \"7161_0\"  \"I went to see this film with a great deal of ...\n","4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Fsc009CDoQRS","colab_type":"code","outputId":"d9084c34-e6fa-4b06-9707-cfc8443bf8bc","executionInfo":{"status":"ok","timestamp":1591075233567,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["dataset.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 2)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"AlgOoD-3oQUQ","colab_type":"code","outputId":"597110a9-46c4-4c59-d559-4563e190cc64","executionInfo":{"status":"ok","timestamp":1591075235239,"user_tz":-330,"elapsed":1053,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["dataset[\"review\"][0]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\\'s release. Life\\'s like that.\"'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"-Q3ZFCcsozWi","colab_type":"code","colab":{}},"source":["## We need to clean the string data before vectorization\n","def clean_string(string):\n","  try:\n","    str=re.sub(r\"^https?:\\/\\/<>.*[\\r\\n]*\", '',string, flags=re.MULTILINE)\n","    str=re.sub(r\"[^A-Za-z]\", \" \", string)\n","    words = str.strip().lower().split()\n","    words = [w for w in words if len(w)>1]\n","    return \" \".join(words)\n","  except:\n","    return \"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTrTZLYlozlK","colab_type":"code","outputId":"7965823c-4112-4cfc-b218-4ee19c701f94","executionInfo":{"status":"ok","timestamp":1591075245825,"user_tz":-330,"elapsed":6789,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["## Clean the review data\n","dataset[\"review_clean\"] = dataset[\"review\"].apply(clean_string)\n","dataset.head() "],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>review</th>\n","      <th>review_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"9999_0\"</td>\n","      <td>\"Watching Time Chasers, it obvious that it was...</td>\n","      <td>watching time chasers it obvious that it was m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"45057_0\"</td>\n","      <td>\"I saw this film about 20 years ago and rememb...</td>\n","      <td>saw this film about years ago and remember it ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"15561_0\"</td>\n","      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n","      <td>minor spoilers br br in new york joan barnard ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"7161_0\"</td>\n","      <td>\"I went to see this film with a great deal of ...</td>\n","      <td>went to see this film with great deal of excit...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"43971_0\"</td>\n","      <td>\"Yes, I agree with everyone on this site this ...</td>\n","      <td>yes agree with everyone on this site this movi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  ...                                       review_clean\n","0   \"9999_0\"  ...  watching time chasers it obvious that it was m...\n","1  \"45057_0\"  ...  saw this film about years ago and remember it ...\n","2  \"15561_0\"  ...  minor spoilers br br in new york joan barnard ...\n","3   \"7161_0\"  ...  went to see this film with great deal of excit...\n","4  \"43971_0\"  ...  yes agree with everyone on this site this movi...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"N0Zkzql4wWa5","colab_type":"code","colab":{}},"source":["## Now Convert the review_clean column into words list document\n","document = []\n","for word in dataset[\"review_clean\"]:\n","  document.append(word.split(\" \"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2OBb7OcKt18","colab_type":"code","outputId":"4b9035a3-bac1-4d54-ef21-e4ab88f51865","executionInfo":{"status":"ok","timestamp":1591075321712,"user_tz":-330,"elapsed":1053,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["print(len(document))\n","print(document[0])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["50000\n","['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'bunch', 'of', 'friends', 'maybe', 'they', 'were', 'sitting', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', 'hey', 'let', 'pool', 'our', 'money', 'together', 'and', 'make', 'really', 'bad', 'movie', 'or', 'something', 'like', 'that', 'what', 'ever', 'they', 'said', 'they', 'still', 'ended', 'up', 'making', 'really', 'bad', 'movie', 'dull', 'story', 'bad', 'script', 'lame', 'acting', 'poor', 'cinematography', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', 'etc', 'all', 'corners', 'were', 'cut', 'except', 'the', 'one', 'that', 'would', 'have', 'prevented', 'this', 'film', 'release', 'life', 'like', 'that']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9KNxXUQnKt4m","colab_type":"code","colab":{}},"source":["## Build the Model\n","model = gensim.models.Word2Vec(document, ## sentences--No of words in documents in the form of list\n","                               min_count=8, ## take only those words which have frequency greater or equal to 10\n","                               workers=4, ## No of CPU\n","                               size=50, ## Embeddeing Size or No of neurons in the hidden layer\n","                               window=5, ##max distance between target and actual words --like left and right distance\n","                               iter=10) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"40BNzpF8Kt7w","colab_type":"code","outputId":"2ff6bf8b-cc9c-43c4-ea4f-ad308cdb02c7","executionInfo":{"status":"ok","timestamp":1591076269723,"user_tz":-330,"elapsed":1107,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["## Now our model in trainned and lets display How many words we have in our trainned Model\n","model.wv.vectors.shape\n","## Model has unique words --31536(size of vocabulary) and each word represented by 50"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31536, 50)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"je3h34_yOmKb","colab_type":"code","colab":{}},"source":["## Lets display the Vocabulary of the Model- unique words-31536\n","#model.wv.vocab"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9S0EzPgOmNB","colab_type":"code","outputId":"fcca1333-9304-45fe-c932-80601c887cf2","executionInfo":{"status":"ok","timestamp":1591076339816,"user_tz":-330,"elapsed":1246,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":223}},"source":["## Lets Display the Embedding of any unique word\n","model.wv[\"watching\"]  ## can see total size is 50"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.7483547 , -1.078792  , -2.413652  ,  2.932797  , -2.697756  ,\n","        1.0881611 , -3.8424175 , -0.19514327, -3.9948156 , -0.5793845 ,\n","       -3.6546783 , -5.3106675 ,  1.7451113 , -0.92208755, -1.1085382 ,\n","        1.9618555 ,  3.200684  ,  3.885689  , -1.7869713 , -4.0070143 ,\n","        1.4089153 , -2.408474  , -2.1207728 ,  1.7571901 ,  4.6971707 ,\n","        2.946527  , -2.180171  ,  0.11515046,  1.1930338 ,  0.28849223,\n","       -0.931327  ,  1.740871  , -0.65453786, -3.2568872 ,  1.997625  ,\n","        1.0446408 ,  1.4336432 ,  2.3780184 , -0.4502919 ,  2.1203218 ,\n","        0.7588038 ,  2.4503772 , -2.298043  , -2.2910674 ,  0.91410625,\n","       -1.8854061 , -2.6668775 ,  0.9612222 ,  3.5591452 ,  1.4377389 ],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"gvV-996mOmPf","colab_type":"code","outputId":"969a66c1-4f1d-45a8-f191-5b097e32d17f","executionInfo":{"status":"ok","timestamp":1591076350849,"user_tz":-330,"elapsed":1154,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["##  Lets display words which have similar meaning\n","model.wv.most_similar(\"perfectly\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('brilliantly', 0.8093279600143433),\n"," ('superbly', 0.7993444204330444),\n"," ('wonderfully', 0.7939373254776001),\n"," ('beautifully', 0.7793017625808716),\n"," ('nicely', 0.756568193435669),\n"," ('expertly', 0.7182832956314087),\n"," ('excellently', 0.6974278688430786),\n"," ('admirably', 0.6648328304290771),\n"," ('perfection', 0.6566228866577148),\n"," ('deftly', 0.6484870910644531)]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"uWmNaGFiZXGD","colab_type":"text"},"source":["We can see we got the most similar words related to perfectly.We only just trained the model and use this.How it is showing such a good result. If we have very large dataset we can even get good result."]},{"cell_type":"code","metadata":{"id":"wrDN0T-bYdph","colab_type":"code","outputId":"b73f36fa-3c73-467f-8e41-7349bcfd799b","executionInfo":{"status":"ok","timestamp":1590831346864,"user_tz":-330,"elapsed":152905,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":131}},"source":["## Lets display those words which are not similar to \"perfectly\"\n","model.wv.doesnt_match([\"fire\",\"water\",\"sea\",\"car\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n","/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'sea'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"OtuGWwSDa0_N","colab_type":"text"},"source":["We can see that we got output \"kitchen\", which is not a similar to other words."]},{"cell_type":"code","metadata":{"id":"kTau-DuvOmRc","colab_type":"code","outputId":"e1773a4e-3731-453a-8aeb-09f65b2fc82c","executionInfo":{"status":"ok","timestamp":1590831346864,"user_tz":-330,"elapsed":152901,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["## Save the model\n","model.save(\"word2vec-movie-imdb\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TujinvkP2j_u","colab_type":"code","outputId":"5bff1d56-34e6-44d9-e934-2d4b74d5f26e","executionInfo":{"status":"ok","timestamp":1590831346865,"user_tz":-330,"elapsed":152897,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["## Lets load model and test\n","trainned_model = gensim.models.Word2Vec.load(\"word2vec-movie-imdb\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PtyMAl2A3pwO","colab_type":"text"},"source":["1. Equation king + man = queen + ?\n","2. In this case there may not be enough data for this equation"]},{"cell_type":"code","metadata":{"id":"OBpYG3CV2kDH","colab_type":"code","outputId":"2bd6278e-a87b-4ced-fba2-89deded92167","executionInfo":{"status":"ok","timestamp":1590831346866,"user_tz":-330,"elapsed":152893,"user":{"displayName":"Atul Patel","photoUrl":"","userId":"07050373690176037615"}},"colab":{"base_uri":"https://localhost:8080/","height":299}},"source":["trainned_model.most_similar(positive=[\"king\",\"queen\"],negative=[\"man\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('askwith', 0.6453505754470825),\n"," ('temple', 0.5604738593101501),\n"," ('hood', 0.5595288276672363),\n"," ('notre', 0.5591071844100952),\n"," ('mansfield', 0.5518171191215515),\n"," ('dallas', 0.5481864809989929),\n"," ('genie', 0.5472075939178467),\n"," ('zeta', 0.5395662784576416),\n"," ('indiana', 0.5339698195457458),\n"," ('kings', 0.5313084721565247)]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"5EP5iTc75bL6","colab_type":"text"},"source":["AS we see that we dont have enough dataset so we did not get expected result. But it is still showing most similar words corresponding to men.\n","\n","We have trained and saved a Word2Vec model for our document. However, when a new dataset comes, you want to update the model so as to account for new words."]},{"cell_type":"markdown","metadata":{"id":"I3PZ59K8DwN0","colab_type":"text"},"source":["One of the challenges with word embedding methods is when we want to obtain vector representations for phrases such as “hot potato” or “Boston Globe”. We can’t just simply combine the individual word vector representations since these phrases don’t represent the combination of meaning of the individual words. And it gets even more complicated when longer phrases and sentences are considered.\n","\n","The other limitation with the word2vec models is that the use of smaller window sizes produce similar embeddings for contrasting words such as “good” and “bad”, which is not desirable especially for tasks where this differentiation is important such as sentiment analysis. Another caveat of word embeddings is that they are dependent on the application in which they are used. Re-training task specific embeddings for every new task is an explored option but this is usually computationally expensive and can be more efficiently addressed using negative sampling. Word2vec models also suffer from other problems such as not taking into account polysemy and other biases that may surface from the training data."]},{"cell_type":"markdown","metadata":{"id":"rNAheJWSQ3hK","colab_type":"text"},"source":["The limits of approaches such as Word2Vec are also important in helping us understand the future trends of NLP research. They set a benchmark for all future research. So where did they come up short?\n","\n","There is only one word embedding per word, i.e. word embeddings can only store one vector for each word. So “bank” only had one meaning for “I lodged money in the bank” and “there is a nice bench on the bank of the river”\n","1. They are difficult to train on large datasets\n","2. You couldn’t fine tune them. To tailor them to your domain you need to train them from scratch\n","3. They are not really a deep neural network. They are trained on a neural network with one hidden layer.\n"]}]}
