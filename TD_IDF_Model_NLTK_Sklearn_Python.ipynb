{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f57f9a",
   "metadata": {},
   "source": [
    "### Tf-IDF Model using Sklearn,nltk and Python\n",
    "\n",
    "In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "The BOW model only considers if a known word occurs in a document or not. It does not care about meaning, context, and order in which they appear.\n",
    "\n",
    "The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
    "\n",
    "Disadvantage is that we are not able to get which word is more important. The BOW model only considers if a known word occurs in a document or not. It does not care about meaning, context, and order in which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50263113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia as wp\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Lets import the TF-IDF Library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa5211",
   "metadata": {},
   "source": [
    "### Lets verify TF and IDF Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8690fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['fred have never been to boston',\n",
    "        'boston is in america',\n",
    "        'paris is the capitol city of france',\n",
    "        'this sentence has no named entities included',\n",
    "        'i have been to san francisco and paris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508a19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = test_sentences_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee82978c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe7077d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fred have never been to boston',\n",
       " 'boston is in america',\n",
       " 'paris is the capitol city of france',\n",
       " 'this sentence has no named entities included',\n",
       " 'i have been to san francisco and paris']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfe560",
   "metadata": {},
   "source": [
    "#### Step 1. Get tfidf scores for boston token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b55f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tftdf_l1 = TfidfVectorizer(encoding='utf-8',\n",
    "    decode_error='strict',\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=None,\n",
    "    tokenizer=None,\n",
    "    analyzer='word',\n",
    "    stop_words=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    vocabulary=None,\n",
    "    binary=False,\n",
    "    norm='l1', # 'l1' or 'l2' \n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=False)\n",
    "# (default settings have smooth_idf=True that adds “1” to the numerator and denominator as if an extra document was seen \n",
    "# containing every term in the collection exactly once, which prevents zero divisions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65e9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc_l1 = tftdf_l1.fit_transform(docs).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb61ec40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['america',\n",
       " 'and',\n",
       " 'been',\n",
       " 'boston',\n",
       " 'capitol',\n",
       " 'city',\n",
       " 'entities',\n",
       " 'france',\n",
       " 'francisco',\n",
       " 'fred',\n",
       " 'has',\n",
       " 'have',\n",
       " 'in',\n",
       " 'included',\n",
       " 'is',\n",
       " 'named',\n",
       " 'never',\n",
       " 'no',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'san',\n",
       " 'sentence',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tfidf_l1 = tftdf_l1.get_feature_names()\n",
    "features_tfidf_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3de45c2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_tfidf_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62cc2be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fred': 9,\n",
       " 'have': 11,\n",
       " 'never': 16,\n",
       " 'been': 2,\n",
       " 'to': 24,\n",
       " 'boston': 3,\n",
       " 'is': 14,\n",
       " 'in': 12,\n",
       " 'america': 0,\n",
       " 'paris': 19,\n",
       " 'the': 22,\n",
       " 'capitol': 4,\n",
       " 'city': 5,\n",
       " 'of': 18,\n",
       " 'france': 7,\n",
       " 'this': 23,\n",
       " 'sentence': 21,\n",
       " 'has': 10,\n",
       " 'no': 17,\n",
       " 'named': 15,\n",
       " 'entities': 6,\n",
       " 'included': 13,\n",
       " 'san': 20,\n",
       " 'francisco': 8,\n",
       " 'and': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tftdf_l1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b94ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>america</th>\n",
       "      <th>and</th>\n",
       "      <th>been</th>\n",
       "      <th>boston</th>\n",
       "      <th>capitol</th>\n",
       "      <th>city</th>\n",
       "      <th>entities</th>\n",
       "      <th>france</th>\n",
       "      <th>francisco</th>\n",
       "      <th>fred</th>\n",
       "      <th>...</th>\n",
       "      <th>named</th>\n",
       "      <th>never</th>\n",
       "      <th>no</th>\n",
       "      <th>of</th>\n",
       "      <th>paris</th>\n",
       "      <th>san</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154346</td>\n",
       "      <td>0.154346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.12199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160586</td>\n",
       "      <td>0.129560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.12956</td>\n",
       "      <td>0.160586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    america       and      been    boston   capitol      city  entities  \\\n",
       "0  0.000000  0.000000  0.154346  0.154346  0.000000  0.000000  0.000000   \n",
       "1  0.276733  0.000000  0.000000  0.223267  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.151204  0.151204  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.142857   \n",
       "4  0.000000  0.160586  0.129560  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     france  francisco      fred  ...     named     never        no        of  \\\n",
       "0  0.000000   0.000000  0.191308  ...  0.000000  0.191308  0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.151204   0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.151204   \n",
       "3  0.000000   0.000000  0.000000  ...  0.142857  0.000000  0.142857  0.000000   \n",
       "4  0.000000   0.160586  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     paris       san  sentence       the      this        to  \n",
       "0  0.00000  0.000000  0.000000  0.000000  0.000000  0.154346  \n",
       "1  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.12199  0.000000  0.000000  0.151204  0.000000  0.000000  \n",
       "3  0.00000  0.000000  0.142857  0.000000  0.142857  0.000000  \n",
       "4  0.12956  0.160586  0.000000  0.000000  0.000000  0.129560  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Craeate the dataframe\n",
    "df_doc_l1 = pd.DataFrame(tfidf_doc_l1,columns=features_tfidf_l1)\n",
    "df_doc_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29baae00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.154346\n",
       "1    0.223267\n",
       "2    0.000000\n",
       "3    0.000000\n",
       "4    0.000000\n",
       "Name: boston, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF for \"boston\"\n",
    "df_doc_l1.loc[:,'boston']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfa1df",
   "metadata": {},
   "source": [
    "#### Step 2.Calculate tfidf for boston token without Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d129e",
   "metadata": {},
   "source": [
    "**TF(t)** = No of times term t appear in document / Total No of terms in the document\n",
    "\n",
    "**IDF(t)** = log((Total No of documents+1) / (Number of documents with term t in it)) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4598f620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fred have never been to boston'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0th document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fee4450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "782ba455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2821911967599909"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the tfidf of 'boston' in 0th document\n",
    "# Note, i does not count as a token according to builtin tokenization scheme.\n",
    "tfidf_boston_wo_norm_0 = (1/6) * (np.log((1+5)/(1+2))+1)\n",
    "tfidf_boston_wo_norm_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2896a39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boston is in america'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st document\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300a45ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42328679513998635"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the tfidf of 'boston' in 1st document\n",
    "# Note, i does not count as a token according to builtin tokenization scheme.\n",
    "tfidf_boston_wo_norm_1 = (1/4) * (np.log((1+5)/(1+2))+1)\n",
    "tfidf_boston_wo_norm_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31831d68",
   "metadata": {},
   "source": [
    "#### Step 3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "372ea9be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fred have never been to boston',\n",
       " 'boston is in america',\n",
       " 'paris is the capitol city of france',\n",
       " 'this sentence has no named entities included',\n",
       " 'i have been to san francisco and paris']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e7b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'boston' tfidf for l1 \n",
      " 0    0.1543\n",
      "1    0.2233\n",
      "2    0.0000\n",
      "3    0.0000\n",
      "4    0.0000\n",
      "Name: boston, dtype: float64\n",
      "tfidf_boston_w_l1_norm_0 :  0.1543\n",
      "tfidf_boston_w_l1_norm_1 :  0.2233\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the l1 normalization first (we will calculate all the td*idf for all the words in a vector(sentence))\n",
    "# All calculated non-normalized tfdid's should sum up to 1 by row:\n",
    "\n",
    "l1_norm_0 = ((1/6) * (np.log((1+5)/(1+1))+1) +\n",
    "         (1/6) * (np.log((1+5)/(1+2))+1) +\n",
    "         (1/6) * (np.log((1+5)/(1+1))+1) +\n",
    "         (1/6) * (np.log((1+5)/(1+2))+1) +\n",
    "         (1/6) * (np.log((1+5)/(1+2))+1)+\n",
    "         (1/6) * (np.log((1+5)/(1+2))+1))\n",
    "\n",
    "l1_norm_1 = ((1/4) * (np.log((1+5)/(1+2))+1) +\n",
    "         (1/4) * (np.log((1+5)/(1+2))+1) +\n",
    "         (1/4) * (np.log((1+5)/(1+1))+1) +\n",
    "         (1/4) * (np.log((1+5)/(1+1))+1))\n",
    "\n",
    "tfidf_boston_w_l1_norm_0 = tfidf_boston_wo_norm_0/l1_norm_0\n",
    "             \n",
    "tfidf_boston_w_l1_norm_1 = tfidf_boston_wo_norm_1/l1_norm_1\n",
    "   \n",
    "# TFIDF for \"boston\"\n",
    "print(\"'boston' tfidf for l1 \\n\",np.round(df_doc_l1.loc[:,'boston'],4))\n",
    "             \n",
    "print(\"tfidf_boston_w_l1_norm_0 : \",np.round(tfidf_boston_w_l1_norm_0,4)) \n",
    "print(\"tfidf_boston_w_l1_norm_1 : \",np.round(tfidf_boston_w_l1_norm_1,4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2e8f3",
   "metadata": {},
   "source": [
    "we are getting the same tfidf score as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f4cd815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fred have never been to boston',\n",
       " 'boston is in america',\n",
       " 'paris is the capitol city of france',\n",
       " 'this sentence has no named entities included',\n",
       " 'i have been to san francisco and paris']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4d30d6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'boston' tfidf for l2 \n",
      " 0    0.376\n",
      "1    0.444\n",
      "2    0.000\n",
      "3    0.000\n",
      "4    0.000\n",
      "Name: boston, dtype: float64\n",
      "tfidf_boston_w_l2_norm_0 :  0.376\n",
      "tfidf_boston_w_l2_norm_1 :  0.444\n"
     ]
    }
   ],
   "source": [
    "# Let's now do the same math for l2 norm.\n",
    "\n",
    "tfidf_l2 = TfidfVectorizer(sublinear_tf=True,norm='l2')\n",
    "tfidf_doc_l2 = tfidf_l2.fit_transform(docs).todense()\n",
    "\n",
    "\n",
    "features_tfidf_l2 = tfidf_l2.get_feature_names()\n",
    "features_tfidf_l2\n",
    "\n",
    "# Craeate the dataframe\n",
    "df_doc_l2 = pd.DataFrame(tfidf_doc_l2,columns=features_tfidf_l2)\n",
    "df_doc_l2\n",
    "\n",
    "#print(df_doc_l2)\n",
    "\n",
    "# TFIDF for \"boston\"\n",
    "print(\"'boston' tfidf for l2 \\n\",np.round(df_doc_l2.loc[:,'boston'],4))\n",
    "\n",
    "l2_norm_0 = np.sqrt(((1/6) * (np.log((1+5)/(1+1))+1))**2 +\n",
    "         ((1/6) * (np.log((1+5)/(1+2))+1))**2 +\n",
    "         ((1/6) * (np.log((1+5)/(1+1))+1))**2 +\n",
    "         ((1/6) * (np.log((1+5)/(1+2))+1))**2 +\n",
    "         ((1/6) * (np.log((1+5)/(1+2))+1))**2+\n",
    "         ((1/6) * (np.log((1+5)/(1+2))+1))**2)\n",
    "                    \n",
    "l2_norm_1 = np.sqrt(((1/4) * (np.log((1+5)/(1+2))+1))**2 +\n",
    "         ((1/4) * (np.log((1+5)/(1+2))+1))**2 +\n",
    "         ((1/4) * (np.log((1+5)/(1+1))+1))**2 +\n",
    "         ((1/4) * (np.log((1+5)/(1+1))+1))**2)\n",
    "\n",
    "\n",
    "tfidf_boston_w_l2_norm_0 = tfidf_boston_wo_norm_0/l2_norm_0\n",
    "             \n",
    "tfidf_boston_w_l2_norm_1 = tfidf_boston_wo_norm_1/l2_norm_1\n",
    "             \n",
    "             \n",
    "print(\"tfidf_boston_w_l2_norm_0 : \",np.round(tfidf_boston_w_l2_norm_0,4)) \n",
    "print(\"tfidf_boston_w_l2_norm_1 : \",np.round(tfidf_boston_w_l2_norm_1,4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d825089",
   "metadata": {},
   "source": [
    "Limitations\n",
    "The main limitation of TF IDF is that word order which is an important part of understanding the meaning of a sentence is not considered in TF-IDF.\n",
    "Also, document length can introduce a lot of variance in the TF IDF values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c82e52",
   "metadata": {},
   "source": [
    "### Lets use some other dataset and first calculate the BOW and then TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c479bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create small paragraph for testing\n",
    "test_paragraph = \"Bill travelled to the 'office' by his car from his house to play the football. Bill reached the office and want to play the football but picked up the 'football' there and left the office. Bill again went to the his friend's house. His friend's name is Fred. After reaching to his friend's house , he gave the football to Fred.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca67bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ea3fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we get the answer of the question \"What did Bill give to Fred?\" by using the BOW model ?\n",
    "# Answer is football\n",
    "# Lets apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bdd707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first convert paragraph into sentences\n",
    "test_sentences = nltk.sent_tokenize(test_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "986fd25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Bill travelled to the 'office' by his car from his house to play the football.\",\n",
       " \"Bill reached the office and want to play the football but picked up the 'football' there and left the office.\",\n",
       " \"Bill again went to the his friend's house.\",\n",
       " \"His friend's name is Fred.\",\n",
       " \"After reaching to his friend's house , he gave the football to Fred.\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e964bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02535d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the sentences\n",
    "# Remove the usefullness characters sentence by sentence\n",
    "test_sentences_clean = []\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences1 = re.sub(\"[^0-9a-zA-Z]+\",' ',test_sentences[i]) # remove all the words excpet alphanemeric\n",
    "    test_sentences2 = test_sentences1.lower().split() # lower and then split the sentences in the words\n",
    "    test_sentences3 = [w for w in test_sentences2 if w not in set(stopwords.words('english'))] # remove stopwords\n",
    "    #test_sentences4 = list(set(test_sentences3)) # remove duplicate in sentences\n",
    "    test_sentences4 = test_sentences3\n",
    "    test_sentences5 = ' '.join(test_sentences4)\n",
    "    test_sentences_clean.append(test_sentences5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7b1027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distinct words in paragraph after cleaning\n",
    "# Build vocabulary\n",
    "def test_words_clean(paragraph):\n",
    "    test_words1 = re.sub(\"[^0-9a-zA-Z]+\",' ',paragraph)\n",
    "    test_words2 = test_words1.lower().split()\n",
    "    test_words3 = [w for w in test_words2 if w not in set(stopwords.words('english'))]\n",
    "    test_words4 = list(set(test_words3))\n",
    "    return test_words4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e532e967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary Size\n",
    "test_words_clean = test_words_clean(test_paragraph)\n",
    "len(test_words_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7f4569e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fred',\n",
       " 'football',\n",
       " 'travelled',\n",
       " 'want',\n",
       " 'picked',\n",
       " 'went',\n",
       " 'bill',\n",
       " 'office',\n",
       " 'play',\n",
       " 'reached',\n",
       " 'friend',\n",
       " 'name',\n",
       " 'gave',\n",
       " 'house',\n",
       " 'left',\n",
       " 'car',\n",
       " 'reaching']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a48d8d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill travelled office car house play football',\n",
       " 'bill reached office want play football picked football left office',\n",
       " 'bill went friend house',\n",
       " 'friend name fred',\n",
       " 'reaching friend house gave football fred']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "994067e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets apply the BOW model\n",
    "# Define the model for oroginal sentences\n",
    "bow_ori = CountVectorizer(stop_words=set(stopwords.words('english')))\n",
    "# Define the model for cleaned sentences\n",
    "bow_clean = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a02ae353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train our original sentences with BOW\n",
    "# Covert into clean test sentences into Matrix of Token Counts\n",
    "X_ori = bow_ori.fit_transform(test_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38350966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train our clean test sentences with BOW\n",
    "# Covert into clean test sentences into Matrix of Token Counts\n",
    "X_clean = bow_clean.fit_transform(test_sentences_clean).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c352dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matix with Original Sentences (5, 17)\n",
      "Shape of Matix with Cleaned Sentences (5, 17)\n"
     ]
    }
   ],
   "source": [
    "# Lets check the shape of the matrix\n",
    "print(\"Shape of Matix with Original Sentences\",X_ori.shape)\n",
    "print(\"Shape of Matix with Cleaned Sentences\",X_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93aa495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 2 0 0 0 0 1 0 2 1 1 1 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b9fb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 2 0 0 0 0 1 0 2 1 1 1 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1524d7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill',\n",
       " 'car',\n",
       " 'football',\n",
       " 'fred',\n",
       " 'friend',\n",
       " 'gave',\n",
       " 'house',\n",
       " 'left',\n",
       " 'name',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'play',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'travelled',\n",
       " 'want',\n",
       " 'went']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the features in Matrix created with Original sentences\n",
    "features_ori = bow_ori.get_feature_names()\n",
    "features_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2e6472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill',\n",
       " 'car',\n",
       " 'football',\n",
       " 'fred',\n",
       " 'friend',\n",
       " 'gave',\n",
       " 'house',\n",
       " 'left',\n",
       " 'name',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'play',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'travelled',\n",
       " 'want',\n",
       " 'went']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the features in Matrix created with cleaned sentences\n",
    "features_clean = bow_clean.get_feature_names()\n",
    "features_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df5c7dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill</th>\n",
       "      <th>car</th>\n",
       "      <th>football</th>\n",
       "      <th>fred</th>\n",
       "      <th>friend</th>\n",
       "      <th>gave</th>\n",
       "      <th>house</th>\n",
       "      <th>left</th>\n",
       "      <th>name</th>\n",
       "      <th>office</th>\n",
       "      <th>picked</th>\n",
       "      <th>play</th>\n",
       "      <th>reached</th>\n",
       "      <th>reaching</th>\n",
       "      <th>travelled</th>\n",
       "      <th>want</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill  car  football  fred  friend  gave  house  left  name  office  picked  \\\n",
       "0     1    1         1     0       0     0      1     0     0       1       0   \n",
       "1     1    0         2     0       0     0      0     1     0       2       1   \n",
       "2     1    0         0     0       1     0      1     0     0       0       0   \n",
       "3     0    0         0     1       1     0      0     0     1       0       0   \n",
       "4     0    0         1     1       1     1      1     0     0       0       0   \n",
       "\n",
       "   play  reached  reaching  travelled  want  went  \n",
       "0     1        0         0          1     0     0  \n",
       "1     1        1         0          0     1     0  \n",
       "2     0        0         0          0     0     1  \n",
       "3     0        0         0          0     0     0  \n",
       "4     0        0         1          0     0     0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for original sentences\n",
    "df_ori = pd.DataFrame(X_ori,columns=features_ori)\n",
    "df_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "756b3ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill</th>\n",
       "      <th>car</th>\n",
       "      <th>football</th>\n",
       "      <th>fred</th>\n",
       "      <th>friend</th>\n",
       "      <th>gave</th>\n",
       "      <th>house</th>\n",
       "      <th>left</th>\n",
       "      <th>name</th>\n",
       "      <th>office</th>\n",
       "      <th>picked</th>\n",
       "      <th>play</th>\n",
       "      <th>reached</th>\n",
       "      <th>reaching</th>\n",
       "      <th>travelled</th>\n",
       "      <th>want</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill  car  football  fred  friend  gave  house  left  name  office  picked  \\\n",
       "0     1    1         1     0       0     0      1     0     0       1       0   \n",
       "1     1    0         2     0       0     0      0     1     0       2       1   \n",
       "2     1    0         0     0       1     0      1     0     0       0       0   \n",
       "3     0    0         0     1       1     0      0     0     1       0       0   \n",
       "4     0    0         1     1       1     1      1     0     0       0       0   \n",
       "\n",
       "   play  reached  reaching  travelled  want  went  \n",
       "0     1        0         0          1     0     0  \n",
       "1     1        1         0          0     1     0  \n",
       "2     0        0         0          0     0     1  \n",
       "3     0        0         0          0     0     0  \n",
       "4     0        0         1          0     0     0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for cleaned sentences\n",
    "df_clean = pd.DataFrame(X_clean,columns=features_clean)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512775b",
   "metadata": {},
   "source": [
    "The length of the vector(X_test.shape[1]) will always be equal to vocabulary size(\"test_words_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e680f296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill travelled office car house play football',\n",
       " 'bill reached office want play football picked football left office',\n",
       " 'bill went friend house',\n",
       " 'friend name fred',\n",
       " 'reaching friend house gave football fred']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089c7ec",
   "metadata": {},
   "source": [
    "Limitations of BOW\n",
    "\n",
    "We can clearly see from the above df_test that our paragraph is cleally well tokenised but there are some disadvantages if we use this to train the ML model. If you see all the words are marked with count of that word in that sentemce so we can not say which words have more weightages\n",
    "in the paragraphs.\n",
    "\n",
    "\n",
    "Semantic meaning: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which it’s used. The same word can be used in multiple places based on the context or nearby words.\n",
    "\n",
    "Vector size: For a large document, the vector size can be huge resulting in a lot of computation and time. You may need to ignore words based on relevance to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11062f7f",
   "metadata": {},
   "source": [
    "### Lets use the TF-IDF to solve the limitaion of BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3092fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the model for original sentences\n",
    "vectorizer_ori = TfidfVectorizer(stop_words=set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a3dbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the model for original sentences\n",
    "vectorizer_clean = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96ddc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train our original sentences with TF-IDF\n",
    "# Covert into clean test sentences into Matrix\n",
    "X_ori_tfidf = vectorizer_ori.fit_transform(test_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df27c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train our clean test sentences with TF-IDF\n",
    "# Covert into clean test sentences into Matrix\n",
    "X_clean_tfidf = vectorizer_clean.fit_transform(test_sentences_clean).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a3392d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matix with Original Sentences (5, 17)\n",
      "Shape of Matix with Cleaned Sentences (5, 17)\n"
     ]
    }
   ],
   "source": [
    "# Lets check the shape of the matrix\n",
    "print(\"Shape of Matix with Original Sentences\",X_ori_tfidf.shape)\n",
    "print(\"Shape of Matix with Cleaned Sentences\",X_clean_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6f538ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill',\n",
       " 'car',\n",
       " 'football',\n",
       " 'fred',\n",
       " 'friend',\n",
       " 'gave',\n",
       " 'house',\n",
       " 'left',\n",
       " 'name',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'play',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'travelled',\n",
       " 'want',\n",
       " 'went']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the features in Matrix created with Original sentences\n",
    "features_ori_tfidf = vectorizer_ori.get_feature_names()\n",
    "features_ori_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f41e36fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill',\n",
       " 'car',\n",
       " 'football',\n",
       " 'fred',\n",
       " 'friend',\n",
       " 'gave',\n",
       " 'house',\n",
       " 'left',\n",
       " 'name',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'play',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'travelled',\n",
       " 'want',\n",
       " 'went']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the features in Matrix created with clean sentences\n",
    "features_clean_tfidf = vectorizer_clean.get_feature_names()\n",
    "features_clean_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dff589f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill</th>\n",
       "      <th>car</th>\n",
       "      <th>football</th>\n",
       "      <th>fred</th>\n",
       "      <th>friend</th>\n",
       "      <th>gave</th>\n",
       "      <th>house</th>\n",
       "      <th>left</th>\n",
       "      <th>name</th>\n",
       "      <th>office</th>\n",
       "      <th>picked</th>\n",
       "      <th>play</th>\n",
       "      <th>reached</th>\n",
       "      <th>reaching</th>\n",
       "      <th>travelled</th>\n",
       "      <th>want</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.46387</td>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.46387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217316</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.434632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523595</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.261798</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556816</td>\n",
       "      <td>0.462208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.403576</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bill      car  football      fred    friend      gave     house  \\\n",
       "0  0.310659  0.46387  0.310659  0.000000  0.000000  0.000000  0.310659   \n",
       "1  0.217316  0.00000  0.434632  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.437287  0.00000  0.000000  0.000000  0.437287  0.000000  0.437287   \n",
       "3  0.000000  0.00000  0.000000  0.556816  0.462208  0.000000  0.000000   \n",
       "4  0.000000  0.00000  0.335004  0.403576  0.335004  0.500222  0.335004   \n",
       "\n",
       "       left      name    office    picked      play   reached  reaching  \\\n",
       "0  0.000000  0.000000  0.374247  0.000000  0.374247  0.000000  0.000000   \n",
       "1  0.324492  0.000000  0.523595  0.324492  0.261798  0.324492  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.690159  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.500222   \n",
       "\n",
       "   travelled      want      went  \n",
       "0    0.46387  0.000000  0.000000  \n",
       "1    0.00000  0.324492  0.000000  \n",
       "2    0.00000  0.000000  0.652948  \n",
       "3    0.00000  0.000000  0.000000  \n",
       "4    0.00000  0.000000  0.000000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for original sentences\n",
    "df_ori_tfidf = pd.DataFrame(X_ori_tfidf,columns=features_ori_tfidf)\n",
    "df_ori_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9501dadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill</th>\n",
       "      <th>car</th>\n",
       "      <th>football</th>\n",
       "      <th>fred</th>\n",
       "      <th>friend</th>\n",
       "      <th>gave</th>\n",
       "      <th>house</th>\n",
       "      <th>left</th>\n",
       "      <th>name</th>\n",
       "      <th>office</th>\n",
       "      <th>picked</th>\n",
       "      <th>play</th>\n",
       "      <th>reached</th>\n",
       "      <th>reaching</th>\n",
       "      <th>travelled</th>\n",
       "      <th>want</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.46387</td>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.46387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217316</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.434632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523595</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.261798</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.324492</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556816</td>\n",
       "      <td>0.462208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.403576</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500222</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bill      car  football      fred    friend      gave     house  \\\n",
       "0  0.310659  0.46387  0.310659  0.000000  0.000000  0.000000  0.310659   \n",
       "1  0.217316  0.00000  0.434632  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.437287  0.00000  0.000000  0.000000  0.437287  0.000000  0.437287   \n",
       "3  0.000000  0.00000  0.000000  0.556816  0.462208  0.000000  0.000000   \n",
       "4  0.000000  0.00000  0.335004  0.403576  0.335004  0.500222  0.335004   \n",
       "\n",
       "       left      name    office    picked      play   reached  reaching  \\\n",
       "0  0.000000  0.000000  0.374247  0.000000  0.374247  0.000000  0.000000   \n",
       "1  0.324492  0.000000  0.523595  0.324492  0.261798  0.324492  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.690159  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.500222   \n",
       "\n",
       "   travelled      want      went  \n",
       "0    0.46387  0.000000  0.000000  \n",
       "1    0.00000  0.324492  0.000000  \n",
       "2    0.00000  0.000000  0.652948  \n",
       "3    0.00000  0.000000  0.000000  \n",
       "4    0.00000  0.000000  0.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for clean sentences\n",
    "df_clean_tfidf = pd.DataFrame(X_clean_tfidf,columns=features_clean_tfidf)\n",
    "df_clean_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b702caf",
   "metadata": {},
   "source": [
    "We can see that in BOW we had word count for each words presented in that sentence but here we have different weightage for each words in that sentence. So this is more meaningfull compare to have word count."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
