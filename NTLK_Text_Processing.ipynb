{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP text processing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = open(\"story.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(754880, str)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story),type(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing the paragraph's sentences\n",
    "## Means convert paragraphs into sentences\n",
    "sentence = nltk.sent_tokenize(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However little known the feelings or views of such a man may be\\n      on his first entering a neighbourhood, this truth is so well\\n      fixed in the minds of the surrounding families, that he is\\n      considered the rightful property of some one or other of their\\n      daughters.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert sentences into words\n",
    "words = nltk.word_tokenize(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 142526)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words),len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff', 'Chapter', '1', 'It', 'is']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming and Lemmatization and stop words\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = open(\"story.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First convert paragraphs into sentences\n",
    "sentences = nltk.sent_tokenize(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create steming object\n",
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to remove stopwords and apply stemming word by word\n",
    "for sent_index in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[sent_index])\n",
    "    stem_words = [stemming.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences[sent_index]=\" \".join(stem_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['howev littl known feel view man may first enter neighbourhood , truth well fix mind surround famili , consid right properti one daughter .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see above that problem with stemmimng is that output words don't have any meaning \n",
    "e.g. howev but actual word is however and same littl and actual word is little\n",
    "So we have another option to overcome this , we will use the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = open(\"story.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_lem = nltk.sent_tokenize(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However little known the feelings or views of such a man may be\\n      on his first entering a neighbourhood, this truth is so well\\n      fixed in the minds of the surrounding families, that he is\\n      considered the rightful property of some one or other of their\\n      daughters.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_lem[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to remove stopwords and apply lemmatizer word by word\n",
    "for sent_index in range(len(sentences_lem)):\n",
    "    words = nltk.word_tokenize(sentences_lem[sent_index])\n",
    "    lemmatizer_words = [lemmitizer.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences_lem[sent_index]=\" \".join(lemmatizer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However little known feeling view man may first entering neighbourhood , truth well fixed mind surrounding family , considered rightful property one daughter .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_lem[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see above that problem with stemmimng is that output words dont have any meaning but using lammatization we have meaningfull words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Whenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm. Hence, Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words. \n",
    " \n",
    "This model can be visualized using a table, which contains the count of words corresponding to the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = open(\"story.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754880"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = story[0:4865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_bow = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bow = []\n",
    "for sent_index in range(len(sentences)):\n",
    "    words = re.sub(\"[^a-zA-Z]\",\" \",sentences[sent_index])\n",
    "    words = words.lower()\n",
    "    words = words.split()\n",
    "    words = [lemmatizer_bow.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    words = \" \".join(words)\n",
    "    sentences_bow.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff      Chapter 1\\n\\n      It is a truth universally acknowledged, that a single man in\\n      possession of a good fortune, must be in want of a wife.',\n",
       " 'However little known the feelings or views of such a man may be\\n      on his first entering a neighbourhood, this truth is so well\\n      fixed in the minds of the surrounding families, that he is\\n      considered the rightful property of some one or other of their\\n      daughters.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:2]  ## original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter truth universally acknowledged single man possession good fortune must want wife',\n",
       " 'however little known feeling view man may first entering neighbourhood truth well fixed mind surrounding family considered rightful property one daughter']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sentence after cleaning\n",
    "sentences_bow[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create the BOW using sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## Now transfor the sentences_bow in to a vector\n",
    "sentences_vector = CountVectorizer().fit_transform(sentences_bow).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 225)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_vector.shape\n",
    "## I have 31 sentences and total unique words is 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      "  0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_vector[0:2,:])\n",
    "## we can see we get 1 corresponding to unique words in each sentences\n",
    "## 1st sentences we have total 12 words and same we have total count of 1 is 12. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequencies are not necessarily the best representation for the text. Common words like \"the\", \"a\", \"to\" are almost always the terms with highest frequency in the text. Thus, having a high raw count does not necessarily mean that the corresponding word is more important. To address this problem, one of the most popular ways to \"normalize\" the term frequencies is to weight a term by the inverse of document frequency, or tf–idf. \n",
    "\n",
    "Also we BOW introduced limitations such as large feature dimension, sparse representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF, which stands for term frequency — inverse document frequency, is a scoring measure widely used in information retrieval (IR) or summarization. TF-IDF is intended to reflect how relevant a term is in a given document.\n",
    "\n",
    "The intuition behind it is that if a word occurs multiple times in a document, we should boost its relevance as it should be more meaningful than other words that appear fewer times (TF). At the same time, if a word occurs many times in a document but also along many other documents, maybe it is because this word is just a frequent word; not because it was relevant or meaningful (IDF).\n",
    "\n",
    "Defining what a “relevant word” means\n",
    "\n",
    "We can come up with a more or less subjective definition driven by our intuition: a word’s relevance is proportional to the amount of information that it gives about its context (a sentence, a document or a full dataset). That is, the most relevant words are those that would help us, as humans, to better understand a whole document without reading it all.\n",
    "\n",
    "How to Compute:\n",
    "tf-idf is a weighting scheme that assigns each term in a document a weight based on its term frequency (tf) and inverse document frequency (idf). The terms with higher weight scores are considered to be more important.\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms-\n",
    "\n",
    "    Normalized Term Frequency (tf)\n",
    "    Inverse Document Frequency (idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = open(\"story.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = story[0:4865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the story text\n",
    "sentences = nltk.sent_tokenize(story)\n",
    "lemmatizer_tfidf = WordNetLemmatizer()\n",
    "sentences_tfidf = []\n",
    "for sent_index in range(len(sentences)):\n",
    "    words = re.sub(\"[^a-zA-Z]\",\" \",sentences[sent_index])\n",
    "    words = words.lower()\n",
    "    words = words.split()\n",
    "    words = [lemmatizer_tfidf.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    words = \" \".join(words)\n",
    "    sentences_tfidf.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create the TFIDF using sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "## Now transfor the sentences_tfidf in to a vector\n",
    "sentences_vector = TfidfVectorizer().fit_transform(sentences_tfidf).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter truth universally acknowledged single man possession good fortune must want wife']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sentence after cleaning\n",
    "sentences_tfidf[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.33880088 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33880088 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.27655214 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.27655214 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25651253 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21430341 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3023877  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25651253 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3023877  0.\n",
      "  0.         0.         0.         0.33880088 0.         0.\n",
      "  0.         0.         0.         0.27655214 0.         0.\n",
      "  0.         0.         0.25651253 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_vector[0:1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that using TFIDF we get different weight to each words although in BOW we have same type of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words and TFIDF representation don’t consider the semantic relation between words, it just focus on count of word and neglect the arrangement in sentence.\n",
    "To overcome this issue we can use word2vec model. We can refer this on my githublik\n",
    "https://github.com/atulpatelDS/NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = open(\"story.txt\",encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_wv = re.sub(r'\\[[0-9]*\\]',' ',story)\n",
    "story_wv = re.sub(r'\\s+',' ',story_wv)\n",
    "story_wv = story_wv.lower()\n",
    "story_wv = re.sub(r'\\d',' ',story_wv)\n",
    "story_wv = re.sub(r'\\s+',' ',story_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' chapter it is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. however little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. “my dear mr. b'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_wv[1:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert story to sentences\n",
    "sentences = nltk.sent_tokenize(story_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff chapter it is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.',\n",
       " 'however little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.',\n",
       " '“my dear mr. bennet,” said his lady to him one day, “have you heard that netherfield park is let at last?” mr. bennet replied that he had not.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_word = [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4659"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeff',\n",
       "  'chapter',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'truth',\n",
       "  'universally',\n",
       "  'acknowledged',\n",
       "  ',',\n",
       "  'that',\n",
       "  'a',\n",
       "  'single',\n",
       "  'man',\n",
       "  'in',\n",
       "  'possession',\n",
       "  'of',\n",
       "  'a',\n",
       "  'good',\n",
       "  'fortune',\n",
       "  ',',\n",
       "  'must',\n",
       "  'be',\n",
       "  'in',\n",
       "  'want',\n",
       "  'of',\n",
       "  'a',\n",
       "  'wife',\n",
       "  '.']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_word[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets remove the stop words\n",
    "for i in range(len(sentences_word)):\n",
    "    sentences_word[i] = [word for word in sentences_word[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4659"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeff',\n",
       "  'chapter',\n",
       "  'truth',\n",
       "  'universally',\n",
       "  'acknowledged',\n",
       "  ',',\n",
       "  'single',\n",
       "  'man',\n",
       "  'possession',\n",
       "  'good',\n",
       "  'fortune',\n",
       "  ',',\n",
       "  'must',\n",
       "  'want',\n",
       "  'wife',\n",
       "  '.']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_word[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the Model\n",
    "model = Word2Vec(sentences_word, ## sentences--No of words in documents in the form of list\n",
    "                               min_count=1, ## take only those words which have frequency greater or equal to 10\n",
    "                               workers=4, ## No of CPU\n",
    "                               size=50, ## Embeddeing Size or No of neurons in the hidden layer\n",
    "                               window=3, ##max distance between target and actual words --like left and right distance\n",
    "                               iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7215, 50)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now our model in trainned and lets display How many words we have in our trainned Model\n",
    "model.wv.vectors.shape\n",
    "## Model has unique words --7215(size of vocabulary) and each word represented by 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets display the Vocabulary of the Model- unique words-7215\n",
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17268741, -0.12061057,  0.18004519,  0.04055322,  0.17896238,\n",
       "        0.27593186,  0.14127517,  0.10682675,  0.07851309, -0.30606648,\n",
       "       -0.05967107, -0.15174407, -0.24112347, -0.02954264,  0.19970766,\n",
       "       -0.05134613,  0.14362542,  0.32420447,  0.06777881, -0.01279176,\n",
       "       -0.0828521 , -0.10296668,  0.02555546, -0.04400079,  0.13299285,\n",
       "       -0.06986132, -0.05113072, -0.19316839,  0.4596822 ,  0.17663167,\n",
       "       -0.03815211,  0.08304726,  0.03584554,  0.03202673,  0.24119832,\n",
       "        0.03388344,  0.10510354,  0.03883573, -0.23845185, -0.07372835,\n",
       "       -0.02375774,  0.23932548, -0.01776404,  0.06423731, -0.19998643,\n",
       "       -0.09625942, -0.11269914,  0.08175147,  0.05636334,  0.0183573 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lets Display the Embedding of any unique word\n",
    "model.wv[\"property\"]  ## can see total size is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('character', 0.9996856451034546),\n",
       " ('making', 0.9996483325958252),\n",
       " ('since', 0.9996414184570312),\n",
       " ('obliged', 0.9996348023414612),\n",
       " ('compliment', 0.99960857629776),\n",
       " ('subject', 0.9996055364608765),\n",
       " ('proved', 0.999605119228363),\n",
       " ('face', 0.9995934963226318),\n",
       " ('sisters', 0.9995932579040527),\n",
       " ('general', 0.9995889067649841)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Lets display words which have similar meaning\n",
    "model.wv.most_similar(\"satisfaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
